{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4043f8e0-d126-4e1a-8652-a831e023086b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366c2cd8-dc86-42b7-addb-2633a3b6a497",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f769051-218d-453b-bf48-e477693eee6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DateType, DecimalType\n",
    "\n",
    "# Define the schema using StructType\n",
    "schema = StructType([\n",
    "    StructField(\"trade_dt\", DateType(), True),\n",
    "    StructField(\"rec_type\", StringType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"exchange\", StringType(), True),\n",
    "    StructField(\"event_tm\", TimestampType(), True),\n",
    "    StructField(\"event_seq_nb\", IntegerType(), True),\n",
    "    StructField(\"arrival_tm\", TimestampType(), True),\n",
    "    StructField(\"trade_pr\", DecimalType(10, 2), True),  # DecimalType(10, 2) is an example; adjust precision and scale as needed\n",
    "    StructField(\"bid_pr\", DecimalType(10, 2), True),  # Adjust precision and scale as needed\n",
    "    StructField(\"bid_size\", IntegerType(), True),\n",
    "    StructField(\"ask_pr\", DecimalType(10, 2), True),  # Adjust precision and scale as needed\n",
    "    StructField(\"ask_size\", IntegerType(), True),\n",
    "    StructField(\"partition\", StringType(), True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af2ddce-4400-4a2d-9af7-aa31df61d799",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.master('local').appName('app').getOrCreate()\n",
    "# spark.conf.set(\n",
    "# \"fs.azure.account.key.dbstorage7ezax62buslq2.blob.core.windows.net\",\n",
    "# \"<your-storage-account-access-key>\"\n",
    "# )\n",
    "# raw =\n",
    "# spark.textFile(\"wasbs://<container-name>@dbstorage7ezax62buslq2.blob.core.w\n",
    "# indows.net/<path_in_container>\")\n",
    "# parsed = raw.map(lambda line: parse_json(line))\n",
    "csv_1 = \"dbfs:/Volumes/data_engineering/default/data/part-00000-5e4ced0a-66e2-442a-b020-347d0df4df8f-c000.txt\"\n",
    "#data = spark.createDataFrame(parsed)\n",
    "data = spark.read.csv(csv_1, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b99f0b-d3bb-414d-8d3d-3e3bcc899f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_1 = \"dbfs:/Volumes/data_engineering/default/data/part-00000-c6c48831-3d45-4887-ba5f-82060885fc6c-c000.txt\"\n",
    "nyc_data = spark.read.json(json_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b80bcf3-29a0-4dbe-a80d-bc9b139c004a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+-----------------+--------+------------+--------------------+----------+--------+------------+--------------------+-----+----+------+----------+\n|           ask_pr|ask_size|           bid_pr|bid_size|event_seq_nb|            event_tm|event_type|exchange|execution_id|             file_tm|price|size|symbol|  trade_dt|\n+-----------------+--------+-----------------+--------+------------+--------------------+----------+--------+------------+--------------------+-----+----+------+----------+\n| 77.9647975908747|     100|76.10016521142818|     100|           1|2020-08-05 09:36:...|         Q|  NASDAQ|        NULL|2020-08-05 09:30:...| NULL|NULL|  SYMA|2020-08-05|\n|75.94452858561046|     100|75.44372945251948|     100|           2|2020-08-05 09:42:...|         Q|  NASDAQ|        NULL|2020-08-05 09:30:...| NULL|NULL|  SYMA|2020-08-05|\n|80.69114407667608|     100|78.84798564828422|     100|           3|2020-08-05 09:48:...|         Q|  NASDAQ|        NULL|2020-08-05 09:30:...| NULL|NULL|  SYMA|2020-08-05|\n|76.16256530811053|     100|74.98336890552693|     100|           4|2020-08-05 09:53:...|         Q|  NASDAQ|        NULL|2020-08-05 09:30:...| NULL|NULL|  SYMA|2020-08-05|\n|77.85512785142082|     100|76.71408448666702|     100|           5|2020-08-05 10:00:...|         Q|  NASDAQ|        NULL|2020-08-05 09:30:...| NULL|NULL|  SYMA|2020-08-05|\n+-----------------+--------+-----------------+--------+------------+--------------------+----------+--------+------------+--------------------+-----+----+------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "nyc_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1292e2-f119-43df-8b87-c8f9aa1b4593",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def parse_csv(line:str):\n",
    "    record_type_pos = 2\n",
    "    record = line.split(\",\")\n",
    "    try:\n",
    "        # [logic to parse records]\n",
    "        if record[record_type_pos] == \"T\":\n",
    "            fields = record\n",
    "            return Row(date=fields[0], datetime=fields[1], record_type=fields[2], symbol=fields[3], timestamp=fields[4], id=int(fields[5]), exchange=fields[6], price1=float(fields[7]), qty1=int(fields[8]), price2=float(fields[9]), qty2=int(fields[10]))\n",
    "            #event = common_event(col1_val, col2_val, ..., \"T\",\"\")\n",
    "            #return event\n",
    "        elif record[record_type_pos] == \"Q\":\n",
    "            fields = record\n",
    "            return Row(date=fields[0], datetime=fields[1], record_type=fields[2], symbol=fields[3], timestamp=fields[4], id=int(fields[5]), exchange=fields[6], price1=float(fields[7]), qty1=int(fields[8]), price2=float(fields[9]), qty2=int(fields[10]))\n",
    "            #event = common_event(col1_val, col2_val, â€¦ , \"Q\",\"\")\n",
    "            #return event\n",
    "    except Exception as e:\n",
    "        # [save record to dummy event in bad partition]\n",
    "        # [fill in the fields as None or empty string]\n",
    "        #return common_event(,,,....,,,,,\"B\",line)\n",
    "        fields = [None]*11\n",
    "        return Row(date=fields[0], datetime=fields[1], record_type=fields[2], symbol=fields[3], timestamp=fields[4], id=int(fields[5]), exchange=fields[6], price1=float(fields[7]), qty1=int(fields[8]), price2=float(fields[9]), qty2=int(fields[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032d6c5c-9d6b-4fd5-b240-69c46b406cb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-08-05,2020-08-05 09:30:00.0,Q,SYMA,2020-08-05 09:34:51.505,1,NYSE,75.30254839137037,100,75.35916738004924,100', '2020-08-05,2020-08-05 09:30:00.0,Q,SYMA,2020-08-05 09:40:52.586,2,NYSE,77.20874619466693,100,78.90918015646369,100', '2020-08-05,2020-08-05 09:30:00.0,Q,SYMA,2020-08-05 09:50:04.681,3,NYSE,77.15973273251218,100,77.3320469411047,100', '2020-08-05,2020-08-05 09:30:00.0,Q,SYMA,2020-08-05 09:57:46.343,4,NYSE,79.29977331004093,100,80.08399307353596,100', '2020-08-05,2020-08-05 09:30:00.0,Q,SYMA,2020-08-05 10:06:50.886,5,NYSE,77.8634951217078,100,78.30821537434917,100']\n[Row(date='2020-08-05', datetime='2020-08-05 09:30:00.0', record_type='Q', symbol='SYMA', timestamp='2020-08-05 09:34:51.505', id=1, exchange='NYSE', price1=75.30254839137037, qty1=100, price2=75.35916738004924, qty2=100), Row(date='2020-08-05', datetime='2020-08-05 09:30:00.0', record_type='Q', symbol='SYMA', timestamp='2020-08-05 09:40:52.586', id=2, exchange='NYSE', price1=77.20874619466693, qty1=100, price2=78.90918015646369, qty2=100), Row(date='2020-08-05', datetime='2020-08-05 09:30:00.0', record_type='Q', symbol='SYMA', timestamp='2020-08-05 09:50:04.681', id=3, exchange='NYSE', price1=77.15973273251218, qty1=100, price2=77.3320469411047, qty2=100), Row(date='2020-08-05', datetime='2020-08-05 09:30:00.0', record_type='Q', symbol='SYMA', timestamp='2020-08-05 09:57:46.343', id=4, exchange='NYSE', price1=79.29977331004093, qty1=100, price2=80.08399307353596, qty2=100), Row(date='2020-08-05', datetime='2020-08-05 09:30:00.0', record_type='Q', symbol='SYMA', timestamp='2020-08-05 10:06:50.886', id=5, exchange='NYSE', price1=77.8634951217078, qty1=100, price2=78.30821537434917, qty2=100)]\n+----------+--------------------+-----------+------+--------------------+---+--------+-----------------+----+-----------------+----+\n|      date|            datetime|record_type|symbol|           timestamp| id|exchange|           price1|qty1|           price2|qty2|\n+----------+--------------------+-----------+------+--------------------+---+--------+-----------------+----+-----------------+----+\n|2020-08-05|2020-08-05 09:30:...|          Q|  SYMA|2020-08-05 09:34:...|  1|    NYSE|75.30254839137037| 100|75.35916738004924| 100|\n|2020-08-05|2020-08-05 09:30:...|          Q|  SYMA|2020-08-05 09:40:...|  2|    NYSE|77.20874619466693| 100|78.90918015646369| 100|\n|2020-08-05|2020-08-05 09:30:...|          Q|  SYMA|2020-08-05 09:50:...|  3|    NYSE|77.15973273251218| 100| 77.3320469411047| 100|\n|2020-08-05|2020-08-05 09:30:...|          Q|  SYMA|2020-08-05 09:57:...|  4|    NYSE|79.29977331004093| 100|80.08399307353596| 100|\n|2020-08-05|2020-08-05 09:30:...|          Q|  SYMA|2020-08-05 10:06:...|  5|    NYSE| 77.8634951217078| 100|78.30821537434917| 100|\n+----------+--------------------+-----------+------+--------------------+---+--------+-----------------+----+-----------------+----+\nonly showing top 5 rows\n\nNone\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "raw = sc.textFile(\"dbfs:/Volumes/data_engineering/default/data/part-00000-5e4ced0a-66e2-442a-b020-347d0df4df8f-c000.txt\")\n",
    "print(raw.take(5))\n",
    "parsed = raw.map(lambda line: parse_csv(line))\n",
    "print(parsed.take(5)) \n",
    "\n",
    "data = spark.createDataFrame(parsed)\n",
    "print(data.show(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0cd256-b29b-4dcf-98e0-424436accd4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"trade_dt\", DateType(), True),\n",
    "    StructField(\"rec_type\", StringType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"exchange\", StringType(), True),\n",
    "    StructField(\"event_tm\", TimestampType(), True),\n",
    "    StructField(\"event_seq_nb\", IntegerType(), True),\n",
    "    StructField(\"arrival_tm\", TimestampType(), True),\n",
    "    StructField(\"trade_pr\", DecimalType(10, 2), True),  # DecimalType(10, 2) is an example; adjust precision and scale as needed\n",
    "    StructField(\"bid_pr\", DecimalType(10, 2), True),  # Adjust precision and scale as needed\n",
    "    StructField(\"bid_size\", IntegerType(), True),\n",
    "    StructField(\"ask_pr\", DecimalType(10, 2), True),  # Adjust precision and scale as needed\n",
    "    StructField(\"ask_size\", IntegerType(), True),\n",
    "    StructField(\"partition\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Reorder this common event function with the data types\n",
    "def common_event(trade_dt, rec_type, symbol, exchange, event_tm, event_seq_nb, arrival_tm, trade_pr, bid_pr, bid_size, ask_pr, ask_size, partition):\n",
    "    \"\"\"\n",
    "    Example common_event function to return a Row object.\n",
    "    Adjust this function as needed to match your requirements.\n",
    "    \"\"\"\n",
    "    return (trade_dt, rec_type, symbol, exchange, event_tm, event_seq_nb, arrival_tm, trade_pr, bid_pr, bid_size, ask_pr, ask_size, partition)\n",
    "\n",
    "def parse_json(line: str):\n",
    "    try:\n",
    "        # Parse the JSON line into a Python dictionary\n",
    "        record = json.loads(line)\n",
    "        record_type = record.get('event_type', '')\n",
    "\n",
    "        # Logic to parse records based on their type\n",
    "        if record_type == \"Q\":\n",
    "            # Get the applicable field values from the JSON record\n",
    "            trade_dt = record.get('trade_dt', None)\n",
    "            rec_type = record.get('rec_type', None)\n",
    "            symbol = record.get('symbol', None)\n",
    "            exchange = record.get('exchange', None)\n",
    "            event_tm = record.get('event_tm', None)\n",
    "            event_seq_nb = record.get('event_seq_nb', None)\n",
    "            arrival_tm = record.get('arrival_tm', None)\n",
    "            trade_pr = record.get('trade_pr', None)\n",
    "            bid_pr = record.get('bid_pr', None)\n",
    "            bid_size = record.get('bid_size', None)\n",
    "            ask_pr = record.get('ask_pr', None)\n",
    "            ask_size = record.get('ask_size', None)\n",
    "            partition = record.get('partition', None)\n",
    "\n",
    "            if not isdigit(ask_size):\n",
    "                ask_size = 0\n",
    "\n",
    "            # Check if any key fields are empty\n",
    "            if not all([trade_dt, file_tm, symbol, event_tm, event_seq_nb, exchange, bid_pr, bid_size, ask_pr, ask_size]):\n",
    "                event = common_event(trade_dt, file_tm, \"Q\", symbol, event_tm, event_seq_nb, exchange, bid_pr, bid_size, ask_pr, ask_size, \"MISSING_FIELDS\", line)\n",
    "            else:\n",
    "                event = common_event(trade_dt, file_tm, \"Q\", symbol, event_tm, event_seq_nb, exchange, bid_pr, bid_size, ask_pr, ask_size, \"VALID\", \"\")\n",
    "            return event\n",
    "\n",
    "        # Handle other event types like \"T\" if needed\n",
    "        elif record_type == \"T\":\n",
    "            # Assuming similar fields for \"T\" type; adjust as necessary\n",
    "            trade_dt = record.get('trade_dt', None)\n",
    "            file_tm = record.get('file_tm', None)\n",
    "            # Add other fields extraction logic here for type \"T\"\n",
    "\n",
    "            # Check if any key fields are empty\n",
    "            if not all([trade_dt, file_tm]):  # Adjust based on required fields\n",
    "                event = common_event(trade_dt, file_tm, \"T\", None, None, None, None, None, None, None, None, \"MISSING_FIELDS\", line)\n",
    "            else:\n",
    "                event = common_event(trade_dt, file_tm, \"T\", None, None, None, None, None, None, None, None, \"VALID\", \"\")\n",
    "            return event\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle parsing exceptions by saving the record to a dummy event in bad partition\n",
    "        print(f\"Error parsing line: {line} - {str(e)}\")\n",
    "        # Fill in the fields as None or empty string\n",
    "        return common_event(None, None, \"B\", None, None, None, None, None, None, None, None, \"ERROR\", line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "800e77fd-b793-49c8-9b31-2b4bef2be47b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_json(line:str):\n",
    "    \n",
    "    record = json.loads(line)\n",
    "    record_type = record.get('event_type', '')\n",
    "    try:\n",
    "        date = record.get('trade_dt', None)\n",
    "        file_tm = record.get('file_tm', None)\n",
    "        symbol = record.get('symbol', None)\n",
    "        event_tm = record.get('event_tm', None)\n",
    "        event_seq_nb = record.get('event_seq_nb', None)\n",
    "        exchange = record.get('exchange', None)\n",
    "        bid_pr = record.get('bid_pr', None)\n",
    "        bid_size = record.get('bid_size', None)\n",
    "        ask_pr = record.get('ask_pr', None)\n",
    "        ask_size = record.get('ask_size', None)\n",
    "        \n",
    "        # [logic to parse records]\n",
    "        if record_type == \"T\":\n",
    "            \n",
    "            return Row(date=date, datetime=file_tm, record_type=record_type, symbol=symbol, timestamp=event_tm, id=event_seq_nb, exchange=exchange, price1=bid_pr, qty1=bid_size, price2=ask_pr, qty2=ask_size)\n",
    "            #event = common_event(col1_val, col2_val, ..., \"T\",\"\")\n",
    "            #return event\n",
    "        elif record_type == \"Q\":\n",
    "            \n",
    "            return Row(date=date, datetime=file_tm, record_type=record_type, symbol=symbol, timestamp=event_tm, id=event_seq_nb, exchange=exchange, price1=bid_pr, qty1=bid_size, price2=ask_pr, qty2=ask_size)\n",
    "            #event = common_event(col1_val, col2_val, â€¦ , \"Q\",\"\")\n",
    "            #return event\n",
    "    except Exception as e:\n",
    "        # [save record to dummy event in bad partition]\n",
    "        # [fill in the fields as None or empty string]\n",
    "        #return common_event(,,,....,,,,,\"B\",line)\n",
    "        fields = [None]*11\n",
    "        return Row(date=fields[0], datetime=fields[1], record_type=fields[2], symbol=fields[3], timestamp=fields[4], id=fields[5], exchange=fields[6], price1=fields[7], qty1=fields[8], price2=fields[9], qty2=fields[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40265f8-fbc9-4278-9e5c-e135dab17031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"trade_dt\":\"2020-08-05\",\"file_tm\":\"2020-08-05 09:30:00.000\",\"event_type\":\"Q\",\"symbol\":\"SYMA\",\"event_tm\":\"2020-08-05 09:36:55.284\",\"event_seq_nb\":1,\"exchange\":\"NASDAQ\",\"bid_pr\":76.10016521142818,\"bid_size\":100,\"ask_pr\":77.9647975908747,\"ask_size\":100}', '{\"trade_dt\":\"2020-08-05\",\"file_tm\":\"2020-08-05 09:30:00.000\",\"event_type\":\"Q\",\"symbol\":\"SYMA\",\"event_tm\":\"2020-08-05 09:42:32.247\",\"event_seq_nb\":2,\"exchange\":\"NASDAQ\",\"bid_pr\":75.44372945251948,\"bid_size\":100,\"ask_pr\":75.94452858561046,\"ask_size\":100}', '{\"trade_dt\":\"2020-08-05\",\"file_tm\":\"2020-08-05 09:30:00.000\",\"event_type\":\"Q\",\"symbol\":\"SYMA\",\"event_tm\":\"2020-08-05 09:48:06.767\",\"event_seq_nb\":3,\"exchange\":\"NASDAQ\",\"bid_pr\":78.84798564828422,\"bid_size\":100,\"ask_pr\":80.69114407667608,\"ask_size\":100}', '{\"trade_dt\":\"2020-08-05\",\"file_tm\":\"2020-08-05 09:30:00.000\",\"event_type\":\"Q\",\"symbol\":\"SYMA\",\"event_tm\":\"2020-08-05 09:53:09.803\",\"event_seq_nb\":4,\"exchange\":\"NASDAQ\",\"bid_pr\":74.98336890552693,\"bid_size\":100,\"ask_pr\":76.16256530811053,\"ask_size\":100}', '{\"trade_dt\":\"2020-08-05\",\"file_tm\":\"2020-08-05 09:30:00.000\",\"event_type\":\"Q\",\"symbol\":\"SYMA\",\"event_tm\":\"2020-08-05 10:00:10.866\",\"event_seq_nb\":5,\"exchange\":\"NASDAQ\",\"bid_pr\":76.71408448666702,\"bid_size\":100,\"ask_pr\":77.85512785142082,\"ask_size\":100}']\nPythonRDD[168] at RDD at PythonRDD.scala:61\n+----------+--------------------+-----------+------+--------------------+---+--------+-----------------+----+-----------------+----+\n|      date|            datetime|record_type|symbol|           timestamp| id|exchange|           price1|qty1|           price2|qty2|\n+----------+--------------------+-----------+------+--------------------+---+--------+-----------------+----+-----------------+----+\n|2020-08-05|2020-08-05 09:30:...|          Q|  SYMA|2020-08-05 09:36:...|  1|  NASDAQ|76.10016521142818| 100| 77.9647975908747| 100|\n|2020-08-05|2020-08-05 09:30:...|          Q|  SYMA|2020-08-05 09:42:...|  2|  NASDAQ|75.44372945251948| 100|75.94452858561046| 100|\n|2020-08-05|2020-08-05 09:30:...|          Q|  SYMA|2020-08-05 09:48:...|  3|  NASDAQ|78.84798564828422| 100|80.69114407667608| 100|\n|2020-08-05|2020-08-05 09:30:...|          Q|  SYMA|2020-08-05 09:53:...|  4|  NASDAQ|74.98336890552693| 100|76.16256530811053| 100|\n|2020-08-05|2020-08-05 09:30:...|          Q|  SYMA|2020-08-05 10:00:...|  5|  NASDAQ|76.71408448666702| 100|77.85512785142082| 100|\n+----------+--------------------+-----------+------+--------------------+---+--------+-----------------+----+-----------------+----+\nonly showing top 5 rows\n\nNone\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "\n",
    "json_1 = \"dbfs:/Volumes/data_engineering/default/data/part-00000-c6c48831-3d45-4887-ba5f-82060885fc6c-c000.txt\"\n",
    "json_raw = sc.textFile(json_1)\n",
    "print(json_raw.take(5))\n",
    "parsed = json_raw.map(lambda line: parse_json(line))\n",
    "print(parsed)\n",
    "data = spark.createDataFrame(parsed)\n",
    "print(data.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a55d6d-372e-42bb-a540-85ff2394e53f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:446)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:446)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:460)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:577)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:57)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:57)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:57)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:57)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:559)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:818)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:844)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:843)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:898)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:691)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:728)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:446)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:446)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:460)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:577)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:57)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:57)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:57)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:57)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:559)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:818)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:844)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:843)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:898)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:691)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29a59a1-45df-4e7f-91e9-b52db3d1050c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "event_type_counts = data.groupBy(\"event_type\").count()\n",
    "display(event_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f931bd-4600-43bf-a815-2c7f6f45bfd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#data.write.partitionBy(\"partition\").mode(\"overwrite\").parquet(\"output_dir\")\n",
    "data.write.partitionBy(\"record_type\").mode(\"overwrite\").parquet(\"output_dir\")\n",
    "#output_dir/partition=T/\n",
    "#output_dir/partition=Q/\n",
    "#output_dir/partition=B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ddb618b-32e5-4f45-8449-ebdb2e3961cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3739635544351247>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m output_dir\u001B[38;5;241m/\u001B[39mrecord_type\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mT\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'output_dir' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'output_dir' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'output_dir' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-3739635544351247>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m output_dir\u001B[38;5;241m/\u001B[39mrecord_type\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mT\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
        "\u001B[0;31mNameError\u001B[0m: name 'output_dir' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir/record_type=='T'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Ingestion",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
